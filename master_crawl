import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time

urls = [
    "https://www.youtube.com",
    "https://www.google.com",
    "https://www.microsoft.com/edge",
    "https://scratch.mit.edu",
    "https://www.wikipedia.org",
    "https://x.com",
    "https://www.instagram.com",
    "https://github.com",
    "https://info.cern.ch"
]

def get_domain(url):
    return urlparse(url).netloc

def crawl(url):
    headers = {
        "User-Agent": "OracleIntelligenceBot/1.0 (polite crawler for research, contact: info@yourdomain.com)"
    }
    try:
        resp = requests.get(url, headers=headers, timeout=5)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        title = soup.title.string.strip() if soup.title else 'No title'
        text = soup.get_text(separator=' ', strip=True)
        first_words = ' '.join(text.split()[:10])  # First 10 words
        print(f"Domain: {get_domain(url)}")
        print(f"Title: {title}")
        print(f"First words: {first_words}")
        print("="*40)
    except Exception as e:
        print(f"Failed to crawl {url}: {e}")

for url in urls:
    crawl(url)
    time.sleep(2)  # Wait 2 seconds between requests to avoid suspicion
